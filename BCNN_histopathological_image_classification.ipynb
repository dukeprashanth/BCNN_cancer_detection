{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Histopathological Image Classification Using Bayesian Convolutional Neural Networks\n",
    "\n",
    "_This work was completed in Aalto University in the spring of 2019 as part of the course CS-E4890 - Deep Learning_![stockphoto](swirls_2_crop.jpeg \"photo by Unsplash user @lurm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "The aim of the project was to apply probabilistic machine learning in the form of Bayesian neural networks to classification of histopathological images in order to achive robust and reliable prediction of the presence of cancer. The probabilistic approach was chosen because medical applications of machine learning, such as diagnostic tools, require us to be able to quantify the uncertainty present in our models. Neural networks, particularly deep neural networks, have garnered much attention from both academia and the public thanks to groundbraking results in many tasks, but they have serious deficiencies when it comes to quantifying the quality of predictions. Unpredictable behaviour highlighted by [adversarial examples](https://openai.com/blog/adversarial-example-research/) present a major problem in applications where it is necessary that the system behave robustly and predictably, like medical technology. Ideally we would like the neural network to not just perform predictions, but to also provide a reliable information about the confidence of those predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian neural networks\n",
    "The motivation behind the development of Bayesian neural networks (BNNs) is precisely to create models that are inherently probabilistic and to allow us to get an idea about the model's confidence in its predictions. As the name suggests, BNNs are a probabilistic models that apply [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference) to the training procedure. While traditional neural networks learn by adjusting the weight parameters of neurons to minimize a chosen loss function, BNNs assign probability distributions to each of the weights. The following illustration by [Shridhar et al.](https://github.com/kumar-shridhar/PyTorch-BayesianCNN/tree/master/Image%20Recognition) shows this difference between traditional and Bayesian neural networks quite nicely.\n",
    "![stockphoto](BayesCNNwithdist.png \"photo by Unsplash user @lurm\") \n",
    "BNNs learn learn by forming the posterior distribution of the weights based on the training data. What this means is that, given a likelihood function $p(\\mathcal D | \\mathbf w)$, we calculate the posterior distribution of the weights using Bayes' theorem\n",
    "\n",
    "$$ p(\\mathbf w | \\mathcal D) = \\frac{p(\\mathcal D | \\mathbf w)p(\\mathbf w)}{p(\\mathcal D)},$$\n",
    "\n",
    "where $\\mathcal D$ is the training data, $\\mathbf w$ is a particular set of weights, and $p(\\mathbf w)$ is a prior on the weights. In practice, we do not calculate the exact posterior distribution, since the term $p(\\mathcal D)$ is generally intractable. We instead use approximations that are explained below. As the weights are described by probability distributions, the output for each sample is also in the form of probability distributions for all the classes. This posterior predictive distribution is evaluated using the formula \n",
    "\n",
    "$$p(y^* | \\mathbf x^*) = \\int p(y^* | \\mathbf x^*, \\mathbf w)p(\\mathbf w | \\mathcal D)\\text d \\mathbf w,$$\n",
    "\n",
    "where $p(y^* | \\mathbf x^*, \\mathbf w)$ is the probability of the model with weights $\\mathbf w$ assigning the label $y^*$ to unseen data example $\\mathbf x^*$. Again, calculating these distributions exactly would be far too computationally intensive, or outright impossible, so we instead use clever mathematical tricks to approximate the distribution of layer activations, and draw samples to give us an estimate of the posterior predictive distribution. By sampling the activations and using them to compute multiple outputs, Bayesian neural networks are in effect a form of [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning). This reduces overfitting and allows us to quantify the uncertainty in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian convolutional neural networks\n",
    "Even though Bayesian neural networks were introduced all the way back in [1995](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf), they failed to gain much popularity due to the difficulty in training them efficiently. It was only quite recently, with the introduction [Bayes by backprop ](https://arxiv.org/abs/1505.05424) (2015), the [local reparametrization trick ](https://arxiv.org/abs/1506.02557) (2015), and finally [Bayesian CNNs](https://github.com/kumar-shridhar/PyTorch-BayesianCNN/blob/master/Paper/BayesianCNN-with-VariationalInference.pdf) (2018), that deep Bayesian neural networks became practical. Instead of evaluating the Bayes' theorem directly, or even using regular [variational inference](http://edwardlib.org/tutorials/bayesian-neural-network), we use a clever method where we perform two convolutional operations, one which seeks to optimize the means of the parameter distributions, and another to optimize the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data used is from a [Kaggle competition](https://www.kaggle.com/c/histopathologic-cancer-detection/overview), which is a slightly modified version of the [PCam](https://github.com/basveeling/pcam) dataset. From the PCam GitHub page:\n",
    "> [The data] consists of 327 680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted [sic] with a binary label indicating presence of metastatic tissue...all splits have a 50/50 balance between positive and negative examples...A positive label indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue. Tumor tissue in the outer region of the patch does not influence the label.\n",
    "\n",
    "The Kaggle version removes duplicates and splits off 25% of the samples into a test set, which brings the number of training samples down to 220 000. The data can be downloaded [here](https://www.kaggle.com/c/histopathologic-cancer-detection/data). I used the Kaggle data, since it is provided in a convinient format, and allows results to be directly compared to the competition submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv which contains the IDs and labels for the training data\n",
    "label_path = 'kaggle/train_labels.csv'\n",
    "\n",
    "# we make a dictionary called labels, which pairs each ID with its label\n",
    "with open(label_path, mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader) # skip the header\n",
    "    labels = {row[0]: int(row[1]) for row in reader}\n",
    "\n",
    "# we also store all the labels in a separate list\n",
    "list_IDs = list(labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Torch Dataset class to load the samples with parallelization\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, list_IDs, labels, transform):\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        filename = '{}.tif'.format(ID)\n",
    "        \n",
    "        filepath = os.path.join('kaggle', 'train', filename)\n",
    "        image = Image.open(filepath)\n",
    "        X = self.transform(image)\n",
    "        y = self.labels[ID]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for expariments use 20% of the samples for testing and validation and 80% for training\n",
    "test_ratio = 0.1\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# transformations applied to samples\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # to prevent overfitting\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "training_set = Dataset(list_IDs, labels, transform=transform)\n",
    "\n",
    "n_train = len(training_set)\n",
    "indices = list(range(n_train))\n",
    "# randomize the order of the samples\n",
    "np.random.shuffle(indices)\n",
    "test_split = int(np.floor(test_ratio * n_train))\n",
    "validation_split = int(np.floor((test_ratio + validation_ratio) * n_train))\n",
    "\n",
    "test_IDs, validation_IDs, training_IDs = indices[:test_split], \\\n",
    "    indices[test_split:validation_split], indices[validation_split:]\n",
    "\n",
    "# change the number of workers based on your CPU\n",
    "loader_params = {'batch_size': 32, 'num_workers': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The latest version (as of 18.05.2019) of at least one of the modules in the PyTorch-BayesianCNN repo have bugs in them that result in nan when calculating losses for Bayesian layers. This is a <a href=\"https://github.com/kumar-shridhar/PyTorch-BayesianCNN/issues/8\">known issue</a> and the author has promised a fix in the future. I used the version 83f5333 which I would recommend before a proper fix is issued.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running the code, make sure this path is correct\n",
    "sys.path.append('../PyTorch-BayesianCNN/Image Recognition/')\n",
    "\n",
    "# the module containing the main implementation\n",
    "import utils.BBBlayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Experiments and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
